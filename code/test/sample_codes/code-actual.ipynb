{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,functions as F\n",
    "from pyspark import StorageLevel\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "conf.set(\"spark.app.name\", \"ComprehensiveSparkJob\") \n",
    "conf.set(\"spark.master\", \"local[*]\")                \n",
    "conf.set(\"spark.driver.memory\", \"4g\")      \n",
    "conf.set(\"spark.driver.cores\", \"1\")                 \n",
    "conf.set(\"spark.ui.port\", \"4040\")                \n",
    "\n",
    "# Executor Settings\n",
    "conf.set(\"spark.executor.memory\", \"2g\")      \n",
    "conf.set(\"spark.executor.cores\", \"2\")               \n",
    "conf.set(\"spark.executor.instances\", \"3\")         \n",
    "\n",
    "\n",
    "conf.set(\"spark.default.parallelism\", \"6\")         \n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"6\")       \n",
    "conf.set(\"spark.task.cpus\", \"1\")  \n",
    "\n",
    "# Data Handling Settings\n",
    "conf.set(\"spark.memory.fraction\", \"0.8\")            \n",
    "conf.set(\"spark.memory.storageFraction\", \"0.5\")     \n",
    "\n",
    "# Serialization Settings\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")  \n",
    "conf.set(\"spark.kryo.registrationRequired\", \"true\")  \n",
    "conf.set(\"spark.kryo.classesToRegister\", \"org.apache.spark.sql.Row\")  \n",
    "\n",
    "# Debugging and Logging\n",
    "conf.set(\"spark.eventLog.enabled\", \"true\")           \n",
    "conf.set(\"spark.eventLog.dir\", \"/root/spark_log/spark-events/\")  \n",
    "conf.set(\"spark.history.fs.logDirectory\", \"/root/spark_log/spark-history/\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json_file_path = \"/root/docker_dataset/json_splits/\"  \n",
    "input_file_list  = rfl(input_json_file_path,\".json\")\n",
    "parquet_file_path = \"/root/docker_dataset/json_splits/table_example_1.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def func_cache_dft_data(dataframe) :\n",
    "    dataframe.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "def func_json_dataframe (input_file:str) :\n",
    "    return  spark.read.option(\"multiline\", \"true\").json(input_file)\n",
    "\n",
    "def func_repartion_dataframe(dataframe, repartion_count) :\n",
    "    return dataframe.repartition(repartion_count)\n",
    " \n",
    "\n",
    "def func_explode_based_on_one_key (input_dataframe,explode_key:str , alias_of_column :str)  :\n",
    "    return  input_dataframe.select(F.explode(F.col(explode_key)).alias(alias_of_column))\n",
    "\n",
    "def func_explode_based_on_depper_root_key (input_dataframe,explode_key:list  , drop_column :str)  :\n",
    "    \"\"\" Always try to assemeble  he json key  based on the root level  in heirarchial order like root -->values-->payload-->commits then the explode_key   value should be [root,values,payload,commits]\"\"\"\n",
    "    column_level_analysis = (\".\").join(explode_key)\n",
    "    return  input_dataframe.select(f\"{explode_key[0]}.*\",F.explode_outer(F.col(column_level_analysis)).alias(column_level_analysis.replace(\".\",\"_\"))).drop(drop_column)\n",
    "\n",
    "\n",
    "def func_get_rdd_num_partitions( dataframe) :\n",
    "    return dataframe.rdd.getNumPartitions()\n",
    "\n",
    "def func_write_data_to_parquet ( file_path, dataframe,write_mode) :\n",
    "    dataframe.write.parquet(file_path,mode=write_mode)\n",
    "\n",
    "\n",
    "def func_read_parquet_file(file_path) :\n",
    "    return spark.read.parquet(file_path)\n",
    "\n",
    "def func_stop_spark () :\n",
    "      spark.stop()\n",
    "\n",
    "\n",
    "def func_remane_column_names (dataframe) :\n",
    "    column_list = [F.col(c).alias(c.replace('.', '_')) for c in dataframe.columns]\n",
    "    for   values in column_list:\n",
    "        print(values)\n",
    "    return  dataframe.select([F.col(c).alias(c.replace('.', '_')) for c in dataframe.columns])\n",
    "\n",
    "\n",
    "def  func_limit_rows_dataframe (dataframe, limit_row_count) :\n",
    "    return dataframe. limit(limit_row_count)\n",
    "\n",
    "def  finc_count_dft_rows(dataframe):\n",
    "     return dataframe.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def func_main () :\n",
    "     processed_dft_row_count= 0 \n",
    "     for file_list in input_file_list :\n",
    "         big_dpart_dft = func_json_dataframe (input_json_file_path+file_list)\n",
    "         big_dpart_dft = func_repartion_dataframe(big_dpart_dft,100)\n",
    "         big_dpart_dft_json_normalised = func_explode_based_on_one_key (big_dpart_dft,\"values\",\"table_example\")\n",
    "         big_dpart_dft_json_normalised = func_explode_based_on_depper_root_key(big_dpart_dft_json_normalised,[\"table_example\",\"entities\"],\"entities\")\n",
    "         func_cache_dft_data(big_dpart_dft_json_normalised)\n",
    "         print(func_get_rdd_num_partitions(big_dpart_dft_json_normalised))\n",
    "         big_dpart_dft_json_norm_renamed  = func_remane_column_names(big_dpart_dft_json_normalised)\n",
    "         processed_dft_row_count += finc_count_dft_rows(big_dpart_dft_json_norm_renamed)\n",
    "         func_write_data_to_parquet(parquet_file_path,big_dpart_dft_json_norm_renamed,\"append\")\n",
    "     return processed_dft_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    # json_input_row_count  = func_main () \n",
    "    big_dpart_parquet = func_read_parquet_file(parquet_file_path)\n",
    "    # parquet_row_count  = finc_count_dft_rows(big_dpart_parquet)\n",
    "    # big_dpart_parquet = func_limit_rows_dataframe(big_dpart_parquet,100)\n",
    "    big_dpart_parquet.show()\n",
    "except  Exception as error :\n",
    "    spark.stop()\n",
    "    raise error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = \"jdbc:postgresql://host.docker.internal:5432/crime_data_la\"\n",
    "db_properties = {\n",
    "    \"user\": \"john_user\",\n",
    "    \"password\": \"abc@12345\"\n",
    "}\n",
    "\n",
    "table_name = \"crime_data.transactions_data\"\n",
    "df_table = spark.read.jdbc(url=db_url, table=table_name, properties=db_properties)\n",
    "df_table.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dpart_parquet.select(big_dpart_parquet[\"table_example_entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:============================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', 'CASE_STATUS', 'EMPLOYER_NAME', 'SOC_NAME', 'JOB_TITLE', 'FULL_TIME_POSITION', 'PREVAILING_WAGE', 'YEAR', 'WORKSITE', 'lon', 'lat']\n",
      "+---+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+--------------------+------------+----------+\n",
      "|_c0|        CASE_STATUS|       EMPLOYER_NAME|            SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|            WORKSITE|         lon|       lat|\n",
      "+---+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+--------------------+------------+----------+\n",
      "|  1|CERTIFIED-WITHDRAWN|UNIVERSITY OF MIC...|BIOCHEMISTS AND B...|POSTDOCTORAL RESE...|                 N|          36067|2016| ANN ARBOR, MICHIGAN| -83.7430378|42.2808256|\n",
      "|  2|CERTIFIED-WITHDRAWN|GOODMAN NETWORKS,...|    CHIEF EXECUTIVES|CHIEF OPERATING O...|                 Y|         242674|2016|        PLANO, TEXAS| -96.6988856|33.0198431|\n",
      "|  3|CERTIFIED-WITHDRAWN|PORTS AMERICA GRO...|    CHIEF EXECUTIVES|CHIEF PROCESS OFF...|                 Y|         193066|2016|JERSEY CITY, NEW ...| -74.0776417|40.7281575|\n",
      "|  4|CERTIFIED-WITHDRAWN|GATES CORPORATION...|    CHIEF EXECUTIVES|REGIONAL PRESIDEN...|                 Y|         220314|2016|    DENVER, COLORADO| -104.990251|39.7392358|\n",
      "|  5|          WITHDRAWN|PEABODY INVESTMEN...|    CHIEF EXECUTIVES|PRESIDENT MONGOLI...|                 Y|       157518.4|2016| ST. LOUIS, MISSOURI| -90.1994042|38.6270025|\n",
      "|  6|CERTIFIED-WITHDRAWN|BURGER KING CORPO...|    CHIEF EXECUTIVES|EXECUTIVE V P, GL...|                 Y|         225000|2016|      MIAMI, FLORIDA| -80.1917902|25.7616798|\n",
      "|  7|CERTIFIED-WITHDRAWN|BT AND MK ENERGY ...|    CHIEF EXECUTIVES|CHIEF OPERATING O...|                 Y|          91021|2016|      HOUSTON, TEXAS| -95.3698028|29.7604267|\n",
      "|  8|CERTIFIED-WITHDRAWN|GLOBO MOBILE TECH...|    CHIEF EXECUTIVES|CHIEF OPERATIONS ...|                 Y|         150000|2016|SAN JOSE, CALIFORNIA|-121.8863286|37.3382082|\n",
      "|  9|CERTIFIED-WITHDRAWN|  ESI COMPANIES INC.|    CHIEF EXECUTIVES|           PRESIDENT|                 Y|         127546|2016|      MEMPHIS, TEXAS|          NA|        NA|\n",
      "| 10|          WITHDRAWN|LESSARD INTERNATI...|    CHIEF EXECUTIVES|           PRESIDENT|                 Y|         154648|2016|    VIENNA, VIRGINIA| -77.2652604|38.9012225|\n",
      "| 11|CERTIFIED-WITHDRAWN|  H.J. HEINZ COMPANY|    CHIEF EXECUTIVES|CHIEF INFORMATION...|                 Y|         182978|2016|PITTSBURGH, PENNS...| -79.9958864|40.4406248|\n",
      "| 12|CERTIFIED-WITHDRAWN|DOW CORNING CORPO...|    CHIEF EXECUTIVES|VICE PRESIDENT AN...|                 Y|         163717|2016|   MIDLAND, MICHIGAN| -84.2472116|43.6155825|\n",
      "| 13|CERTIFIED-WITHDRAWN|    ACUSHNET COMPANY|    CHIEF EXECUTIVES|   TREASURER AND COO|                 Y|       203860.8|2016|FAIRHAVEN, MASSAC...|          NA|        NA|\n",
      "| 14|CERTIFIED-WITHDRAWN|       BIOCAIR, INC.|    CHIEF EXECUTIVES|CHIEF COMMERCIAL ...|                 Y|         252637|2016|      MIAMI, FLORIDA| -80.1917902|25.7616798|\n",
      "| 15|CERTIFIED-WITHDRAWN|NEWMONT MINING CO...|    CHIEF EXECUTIVES|        BOARD MEMBER|                 Y|         105914|2016|GREENWOOD VILLAGE...|-104.9508141|39.6172101|\n",
      "| 16|CERTIFIED-WITHDRAWN|        VRICON, INC.|    CHIEF EXECUTIVES|CHIEF FINANCIAL O...|                 Y|         153046|2016|  STERLING, VIRGINIA| -77.4291298|39.0066993|\n",
      "| 17|CERTIFIED-WITHDRAWN|CARDIAC SCIENCE C...|  FINANCIAL MANAGERS|VICE PRESIDENT OF...|                 Y|          90834|2016| WAUKESHA, WISCONSIN| -88.2314813|43.0116784|\n",
      "| 18|CERTIFIED-WITHDRAWN|WESTFIELD CORPORA...|    CHIEF EXECUTIVES|GENERAL MANAGER, ...|                 Y|         164050|2016|LOS ANGELES, CALI...|-118.2436849|34.0522342|\n",
      "| 19|          CERTIFIED|      QUICKLOGIX LLC|    CHIEF EXECUTIVES|                 CEO|                 Y|         187200|2016|SANTA CLARA, CALI...|-121.9552356|37.3541079|\n",
      "| 20|          CERTIFIED|MCCHRYSTAL GROUP,...|    CHIEF EXECUTIVES|PRESIDENT, NORTHE...|                 Y|         241842|2016|ALEXANDRIA, VIRGINIA| -77.0469214|38.8048355|\n",
      "+---+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+--------------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/07 20:25:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , CASE_STATUS, EMPLOYER_NAME, SOC_NAME, JOB_TITLE, FULL_TIME_POSITION, PREVAILING_WAGE, YEAR, WORKSITE, lon, lat\n",
      " Schema: _c0, CASE_STATUS, EMPLOYER_NAME, SOC_NAME, JOB_TITLE, FULL_TIME_POSITION, PREVAILING_WAGE, YEAR, WORKSITE, lon, lat\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///root/docker_dataset/h1b_data/h1b16.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV with PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/root/docker_dataset/h1b_data/h1b17.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# List of columns to exclude\n",
    "exclude_columns = ['column_to_exclude1', 'column_to_exclude2']\n",
    "\n",
    "# Select only the columns you want to keep\n",
    "selected_columns = [col for col in df.columns if col not in exclude_columns]\n",
    "\n",
    "print(selected_columns)\n",
    "df_selected = df.select(*selected_columns)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df_selected.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_learning_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
