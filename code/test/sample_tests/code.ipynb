{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/25 20:46:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/25 20:46:47 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "25/01/25 20:46:56 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------+--------------------+--------+--------------------+--------------------+--------+---------------+-----------------+--------------------+--------------------+--------------------+-----------------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------+-------------+--------------------+-----------+---------+---------+------------------+--------------------+--------------------+\n",
      "|  event_id|      event_type|is_public|     event_timestamp| repo_id|           repo_name|            repo_url|actor_id|    actor_login|actor_gravatar_id|   actor_profile_url|    actor_avatar_url|          commit_sha|               commit_message|commit_distinct|          commit_url|        author_email|        author_name|                 ref|ref_type|master_branch|    repo_description|pusher_type|  push_id|push_size|distinct_push_size|         commit_head|       commit_before|\n",
      "+----------+----------------+---------+--------------------+--------+--------------------+--------------------+--------+---------------+-----------------+--------------------+--------------------+--------------------+-----------------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------+-------------+--------------------+-----------+---------+---------+------------------+--------------------+--------------------+\n",
      "|2489651045|     CreateEvent|     true|2015-01-01T15:00:00Z|28688495|       petroav/6.828|https://api.githu...|  665991|        petroav|                 |https://api.githu...|https://avatars.g...|                null|                         null|           null|                null|                null|               null|              master|  branch|       master|Solution to homew...|       user|     null|     null|              null|                null|                null|\n",
      "|2489651051|       PushEvent|     true|2015-01-01T15:00:01Z|28671719|     rspt/rspt-theme|https://api.githu...| 3854017|           rspt|                 |https://api.githu...|https://avatars.g...|6b089eb4a43f728f0...|         Fix main header h...|           true|https://api.githu...|5c682c2d1ec4073e2...|               rspt|   refs/heads/master|    null|         null|                null|       null|536863970|        1|                 1|6b089eb4a43f728f0...|437c03652caa0bc4a...|\n",
      "|2489651053|       PushEvent|     true|2015-01-01T15:00:01Z|28270952|izuzero/xe-module...|https://api.githu...| 6339799|        izuzero|                 |https://api.githu...|https://avatars.g...|ec819b9df4fe612bb...|#20 게시글 및 댓글 삭제 시...|           true|https://api.githu...|df05f55543db3c62c...|         Eunsoo Lee|  refs/heads/develop|    null|         null|                null|       null|536863972|        1|                 1|ec819b9df4fe612bb...|590433109f221a96c...|\n",
      "|2489651057|      WatchEvent|     true|2015-01-01T15:00:03Z| 2871998|   visionmedia/debug|https://api.githu...| 6894991|SametSisartenep|                 |https://api.githu...|https://avatars.g...|                null|                         null|           null|                null|                null|               null|                null|    null|         null|                null|       null|     null|     null|              null|                null|                null|\n",
      "|2489651062|       PushEvent|     true|2015-01-01T15:00:03Z|28593843|   winterbe/streamjs|https://api.githu...|  485033|       winterbe|                 |https://api.githu...|https://avatars.g...|15b303203be31bd29...|         Add comparator su...|           true|https://api.githu...|52a47bffd52d9cea1...|Benjamin Winterberg|   refs/heads/master|    null|         null|                null|       null|536863975|        1|                 1|15b303203be31bd29...|0fef99f604154ccfe...|\n",
      "|2489651063|       PushEvent|     true|2015-01-01T15:00:03Z|27826205|hermanwahyudi/sel...|https://api.githu...| 4319954|  hermanwahyudi|                 |https://api.githu...|https://avatars.g...|1b58dd4c4e14ea9cf...|             Update README.md|          false|https://api.githu...|2bb20d8a71fb7adbc...|             Herman|   refs/heads/master|    null|         null|                null|       null|536863976|        1|                 0|1b58dd4c4e14ea9cf...|20b10e3a605bd177e...|\n",
      "|2489651064|       PushEvent|     true|2015-01-01T15:00:03Z|28682546|jdilt/jdilt.githu...|https://api.githu...| 2881602|          jdilt|                 |https://api.githu...|https://avatars.g...|d13cbd1e5c68b189f...|         refine index page...|           true|https://api.githu...|3e9bbe622d800410f...|              jdilt|   refs/heads/master|    null|         null|                null|       null|536863977|        1|                 1|d13cbd1e5c68b189f...|8515c4a9efb403326...|\n",
      "|2489651066|       PushEvent|     true|2015-01-01T15:00:04Z|24147122| sundaymtn/waterline|https://api.githu...| 3495129|      sundaymtn|                 |https://api.githu...|https://avatars.g...|2a2ec35bfefb9341b...|         Thu Jan  1 10:00:...|           true|https://api.githu...|7fbc091194a9488bf...|        Seth Carter|   refs/heads/master|    null|         null|                null|       null|536863979|        1|                 1|2a2ec35bfefb9341b...|a7dba8faf22d2f342...|\n",
      "|2489651067|       PushEvent|     true|2015-01-01T15:00:04Z|28686619|    zhouzhi2015/temp|https://api.githu...|10363514|    zhouzhi2015|                 |https://api.githu...|https://avatars.g...|22019c081480435bb...|                         测测|           true|https://api.githu...|421c4f4cb8c7fe07e...|  1184795629@qq.com|   refs/heads/master|    null|         null|                null|       null|536863980|        1|                 1|22019c081480435bb...|d5926ef8c6a8a4372...|\n",
      "|2489651071|    ReleaseEvent|     true|2015-01-01T15:00:05Z|20029610| petrkutalek/png2pos|https://api.githu...| 7659931|    petrkutalek|                 |https://api.githu...|https://avatars.g...|                null|                         null|           null|                null|                null|               null|                null|    null|         null|                null|       null|     null|     null|              null|                null|                null|\n",
      "|2489651077|       PushEvent|     true|2015-01-01T15:00:05Z|20469468|caleb-eades/Minec...|https://api.githu...| 4070158|    caleb-eades|                 |https://api.githu...|https://avatars.g...|6ea9a1f5b0b3c4204...|         Auto Snapshot Ser...|           true|https://api.githu...|5bbfe2c07a3ef0b22...|        caleb-eades|   refs/heads/master|    null|         null|                null|       null|536863983|        1|                 1|6ea9a1f5b0b3c4204...|8e94c95939b8f7db4...|\n",
      "|2489651078|      WatchEvent|     true|2015-01-01T15:00:05Z| 5569958|phpsysinfo/phpsys...|https://api.githu...|  285289|       comcxx11|                 |https://api.githu...|https://avatars.g...|                null|                         null|           null|                null|                null|               null|                null|    null|         null|                null|       null|     null|     null|              null|                null|                null|\n",
      "|2489651080|      WatchEvent|     true|2015-01-01T15:00:05Z|25873041|wasabeef/awesome-...|https://api.githu...| 1757814|        Soufien|                 |https://api.githu...|https://avatars.g...|                null|                         null|           null|                null|                null|               null|                null|    null|         null|                null|       null|     null|     null|              null|                null|                null|\n",
      "|2489651083|       PushEvent|     true|2015-01-01T15:00:05Z|26101634|ktgw0316/LightZon...|https://api.githu...| 9538449|       hcremers|                 |https://api.githu...|https://avatars.g...|0fca01b12e6a8a1c5...|         Translated by hcr...|           true|https://api.githu...|8800578b51f022c8d...|               hans|   refs/heads/master|    null|         null|                null|       null|536863987|        1|                 1|0fca01b12e6a8a1c5...|fe610605ba48a87ee...|\n",
      "|2489651087|PullRequestEvent|     true|2015-01-01T15:00:06Z| 3542607|leethomason/tinyxml2|https://api.githu...| 1277095|      Dmitry-Me|                 |https://api.githu...|https://avatars.g...|                null|                         null|           null|                null|                null|               null|                null|    null|         null|                null|       null|     null|     null|              null|                null|                null|\n",
      "|2489651090|PullRequestEvent|     true|2015-01-01T15:00:06Z|10391073|    edx/edx-platform|https://api.githu...| 2362917|    chrisndodge|                 |https://api.githu...|https://avatars.g...|                null|                         null|           null|                null|                null|               null|                null|    null|         null|                null|       null|     null|     null|              null|                null|                null|\n",
      "|2489651091|     IssuesEvent|     true|2015-01-01T15:00:06Z|28594770| yhoonkim/GraphBoard|https://api.githu...| 6269456|       yhoonkim|                 |https://api.githu...|https://avatars.g...|                null|                         null|           null|                null|                null|               null|                null|    null|         null|                null|       null|     null|     null|              null|                null|                null|\n",
      "|2489651093|      WatchEvent|     true|2015-01-01T15:00:06Z|10635999|   dartist/raytracer|https://api.githu...|  546301|      unicomp21|                 |https://api.githu...|https://avatars.g...|                null|                         null|           null|                null|                null|               null|                null|    null|         null|                null|       null|     null|     null|              null|                null|                null|\n",
      "|2489651095|     CreateEvent|     true|2015-01-01T15:00:07Z|19470044|      SundeepK/Clone|https://api.githu...| 2339563|       SundeepK|                 |https://api.githu...|https://avatars.g...|                null|                         null|           null|                null|                null|               null|wip/refactor_leve...|  branch|       master|Clone is a physic...|       user|     null|     null|              null|                null|                null|\n",
      "|2489651096|PullRequestEvent|     true|2015-01-01T15:00:08Z|28668460|       mevlan/script|https://api.githu...|10357835|         mevlan|                 |https://api.githu...|https://avatars.g...|                null|                         null|           null|                null|                null|               null|                null|    null|         null|                null|       null|     null|     null|              null|                null|                null|\n",
      "+----------+----------------+---------+--------------------+--------+--------------------+--------------------+--------+---------------+-----------------+--------------------+--------------------+--------------------+-----------------------------+---------------+--------------------+--------------------+-------------------+--------------------+--------+-------------+--------------------+-----------+---------+---------+------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession,functions as F\n",
    "from pyspark import StorageLevel\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, ArrayType\n",
    "\n",
    "# Step 1: Define SparkConf and Set Configurations\n",
    "conf = SparkConf()\n",
    "\n",
    "# General Application and Driver Settings\n",
    "conf.set(\"spark.app.name\", \"ComprehensiveSparkJob-John Learning \")  # Application name\n",
    "conf.set(\"spark.master\", \"local[*]\")                # Run Spark locally with all available cores\n",
    "conf.set(\"spark.driver.memory\", \"4g\")               # Memory allocated to the driver program\n",
    "conf.set(\"spark.driver.cores\", \"2\")                 # Number of CPU cores for the driver\n",
    "conf.set(\"spark.ui.port\", \"4040\")                   # Web UI port (default is 4040)\n",
    "conf.set(\"spark.executor.memoryOverhead\", \"1g\")\n",
    "\n",
    "\n",
    "# # Executor Settings\n",
    "conf.set(\"spark.executor.memory\", \"4g\")             # Memory allocated per executor\n",
    "conf.set(\"spark.executor.cores\", \"2\")               # Number of cores per executor\n",
    "conf.set(\"spark.executor.instances\", \"2\")           # Number of executor instances\n",
    "\n",
    "\n",
    "conf.set(\"spark.default.parallelism\", \"200\")          # Default parallelism (number of partitions)\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"200\")       # Partitions for shuffle operations\n",
    "# conf.set(\"spark.task.cpus\", \"1\")                    # Number of CPUs allocated per task\n",
    "\n",
    "# # Data Handling Settings\n",
    "conf.set(\"spark.memory.fraction\", \"0.6\")            # Fraction of JVM heap for execution and storage\n",
    "conf.set(\"spark.memory.storageFraction\", \"0.4\")     # Fraction of memory for caching data\n",
    "\n",
    "# # Serialization Settingsk\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")  # Use Kryo serialization\n",
    "conf.set(\"spark.kryo.registrationRequired\", \"false\")   # Ensure classes are registered for Kryo\n",
    "conf.set(\"spark.kryo.classesToRegister\", \"org.apache.spark.sql.Row\")  # Example: Registering a class\n",
    "\n",
    "# # Debugging and Logging\n",
    "conf.set(\"spark.eventLog.enabled\", \"true\")           # Enable event logging\n",
    "conf.set(\"spark.eventLog.dir\", \"/root/spark_log/spark-events/\")  # Directory for event logs\n",
    "conf.set(\"spark.history.fs.logDirectory\", \"/root/spark_log/spark-history/\")  # Spark History Server logs\n",
    "conf.set(\"spark.local.dir\", \"/root/spark_log/spark_cache/\")  # Specify a custom directory\n",
    "\n",
    "conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "conf.set(\"spark.memory.offHeap.size\", \"2g\")  # Adjust based on available memory\n",
    "conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -Xmx5g -Xms5g\")\n",
    "conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/root/spark_logspark_heap_dump.hprof\")\n",
    "conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/root/spark_log/gc.log\")\n",
    "conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\")\n",
    "\n",
    "# Step 2: Initialize SparkSession with Configurations\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "\n",
    "file_path = \"/root/docker_dataset/large-file.json\"  # Replace with the path to your JSON file\n",
    "df = spark.read.option(\"multiline\", \"true\").json(file_path)\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "print(df.rdd.getNumPartitions())  # Check how many partitions are there\n",
    "df = df.repartition(10)\n",
    "# Show the schema to understand the structure\n",
    "\n",
    "\n",
    "# Explode the 'values' array to flatten the structure\n",
    "df_exploded = df.select(F.explode(F.col(\"values\")).alias(\"value\"))\n",
    "\n",
    "commit_schema = ArrayType(StructType([\n",
    "    StructField(\"sha\", StringType(), True),\n",
    "    StructField(\"author\", StructType([\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"distinct\", BooleanType(), True),\n",
    "    StructField(\"url\", StringType(), True)\n",
    "]))\n",
    "\n",
    "# Explode the 'entities' array inside each 'value'\n",
    "df_exploded = df_exploded.select(\n",
    "    \"value.*\", \n",
    "    F.explode_outer(F.col(\"value.payload.commits\")).alias(\"commits\")\n",
    ")\n",
    "\n",
    "\n",
    "df_exploded.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# df_exploded.printSchema()\n",
    "print(df_exploded.rdd.getNumPartitions())  # Check how many partitions are there\n",
    "\n",
    "# Now, you can select the specific columns you're interested in\n",
    "df_final = df_exploded.select(\n",
    "    F.col(\"id\").alias(\"event_id\"),\n",
    "    F.col(\"type\").alias(\"event_type\"),\n",
    "    F.col(\"public\").alias(\"is_public\"),\n",
    "    F.col(\"created_at\").alias(\"event_timestamp\"),    \n",
    "    # Repository details\n",
    "    F.col(\"repo.id\").alias(\"repo_id\"),\n",
    "    F.col(\"repo.name\").alias(\"repo_name\"),\n",
    "    F.col(\"repo.url\").alias(\"repo_url\"),    \n",
    "    # Actor details\n",
    "    F.col(\"actor.id\").alias(\"actor_id\"),\n",
    "    F.col(\"actor.login\").alias(\"actor_login\"),\n",
    "    F.col(\"actor.gravatar_id\").alias(\"actor_gravatar_id\"),\n",
    "    F.col(\"actor.url\").alias(\"actor_profile_url\"),\n",
    "    F.col(\"actor.avatar_url\").alias(\"actor_avatar_url\"),  \n",
    "    # Commit details (after explode)\n",
    "    F.col(\"commits.sha\").alias(\"commit_sha\"),\n",
    "    F.col(\"commits.message\").alias(\"commit_message\"),\n",
    "    F.col(\"commits.distinct\").alias(\"commit_distinct\"),\n",
    "    F.col(\"commits.url\").alias(\"commit_url\"),    \n",
    "    # Author details inside commits\n",
    "    F.col(\"commits.author.email\").alias(\"author_email\"),\n",
    "    F.col(\"commits.author.name\").alias(\"author_name\"),\n",
    "    # Payload details\n",
    "    F.col(\"payload.ref\").alias(\"ref\"),\n",
    "    F.col(\"payload.ref_type\").alias(\"ref_type\"),\n",
    "    F.col(\"payload.master_branch\").alias(\"master_branch\"),\n",
    "    F.col(\"payload.description\").alias(\"repo_description\"),\n",
    "    F.col(\"payload.pusher_type\").alias(\"pusher_type\"),\n",
    "    F.col(\"payload.push_id\").alias(\"push_id\"),\n",
    "    F.col(\"payload.size\").alias(\"push_size\"),\n",
    "    F.col(\"payload.distinct_size\").alias(\"distinct_push_size\"),\n",
    "    F.col(\"payload.head\").alias(\"commit_head\"),\n",
    "    F.col(\"payload.before\").alias(\"commit_before\") \n",
    ")\n",
    "\n",
    "df_final.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df_final.write.parquet(\"/root/docker_dataset/big_data.parquet\",mode=\"overwrite\")\n",
    "df_parquet = spark.read.parquet(\"/root/docker_dataset/big_data.parquet\")\n",
    "df_parquet =df_parquet.limit(100)\n",
    "\n",
    "df_parquet.show()\n",
    "\n",
    "# Step 5: Stop the Spark Session\n",
    "# spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_learning_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
