{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,functions as F\n",
    "from pyspark import StorageLevel\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "conf.set(\"spark.app.name\", \"ComprehensiveSparkJob\") \n",
    "conf.set(\"spark.master\", \"local[*]\")                \n",
    "conf.set(\"spark.driver.memory\", \"4g\")      \n",
    "conf.set(\"spark.driver.cores\", \"1\")                 \n",
    "conf.set(\"spark.ui.port\", \"4040\")                \n",
    "\n",
    "# Executor Settings\n",
    "conf.set(\"spark.executor.memory\", \"2g\")      \n",
    "conf.set(\"spark.executor.cores\", \"2\")               \n",
    "conf.set(\"spark.executor.instances\", \"3\")         \n",
    "\n",
    "\n",
    "conf.set(\"spark.default.parallelism\", \"6\")         \n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"6\")       \n",
    "conf.set(\"spark.task.cpus\", \"1\")  \n",
    "\n",
    "# Data Handling Settings\n",
    "conf.set(\"spark.memory.fraction\", \"0.8\")            \n",
    "conf.set(\"spark.memory.storageFraction\", \"0.5\")     \n",
    "\n",
    "# Serialization Settings\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")  \n",
    "conf.set(\"spark.kryo.registrationRequired\", \"true\")  \n",
    "conf.set(\"spark.kryo.classesToRegister\", \"org.apache.spark.sql.Row\")  \n",
    "\n",
    "# Debugging and Logging\n",
    "conf.set(\"spark.eventLog.enabled\", \"true\")           \n",
    "conf.set(\"spark.eventLog.dir\", \"/root/spark_log/spark-events/\")  \n",
    "conf.set(\"spark.history.fs.logDirectory\", \"/root/spark_log/spark-history/\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json_file_path = \"/root/docker_dataset/json_splits/\"  \n",
    "input_file_list  = rfl(input_json_file_path,\".json\")\n",
    "parquet_file_path = \"/root/docker_dataset/json_splits/table_example_1.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def func_cache_dft_data(dataframe) :\n",
    "    dataframe.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "def func_json_dataframe (input_file:str) :\n",
    "    return  spark.read.option(\"multiline\", \"true\").json(input_file)\n",
    "\n",
    "def func_repartion_dataframe(dataframe, repartion_count) :\n",
    "    return dataframe.repartition(repartion_count)\n",
    " \n",
    "\n",
    "def func_explode_based_on_one_key (input_dataframe,explode_key:str , alias_of_column :str)  :\n",
    "    return  input_dataframe.select(F.explode(F.col(explode_key)).alias(alias_of_column))\n",
    "\n",
    "def func_explode_based_on_depper_root_key (input_dataframe,explode_key:list  , drop_column :str)  :\n",
    "    \"\"\" Always try to assemeble  he json key  based on the root level  in heirarchial order like root -->values-->payload-->commits then the explode_key   value should be [root,values,payload,commits]\"\"\"\n",
    "    column_level_analysis = (\".\").join(explode_key)\n",
    "    return  input_dataframe.select(f\"{explode_key[0]}.*\",F.explode_outer(F.col(column_level_analysis)).alias(column_level_analysis.replace(\".\",\"_\"))).drop(drop_column)\n",
    "\n",
    "\n",
    "def func_get_rdd_num_partitions( dataframe) :\n",
    "    return dataframe.rdd.getNumPartitions()\n",
    "\n",
    "def func_write_data_to_parquet ( file_path, dataframe,write_mode) :\n",
    "    dataframe.write.parquet(file_path,mode=write_mode)\n",
    "\n",
    "\n",
    "def func_read_parquet_file(file_path) :\n",
    "    return spark.read.parquet(file_path)\n",
    "\n",
    "def func_stop_spark () :\n",
    "      spark.stop()\n",
    "\n",
    "\n",
    "def func_remane_column_names (dataframe) :\n",
    "    column_list = [F.col(c).alias(c.replace('.', '_')) for c in dataframe.columns]\n",
    "    for   values in column_list:\n",
    "        print(values)\n",
    "    return  dataframe.select([F.col(c).alias(c.replace('.', '_')) for c in dataframe.columns])\n",
    "\n",
    "\n",
    "def  func_limit_rows_dataframe (dataframe, limit_row_count) :\n",
    "    return dataframe. limit(limit_row_count)\n",
    "\n",
    "def  finc_count_dft_rows(dataframe):\n",
    "     return dataframe.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def func_main () :\n",
    "     processed_dft_row_count= 0 \n",
    "     for file_list in input_file_list :\n",
    "         big_dpart_dft = func_json_dataframe (input_json_file_path+file_list)\n",
    "         big_dpart_dft = func_repartion_dataframe(big_dpart_dft,100)\n",
    "         big_dpart_dft_json_normalised = func_explode_based_on_one_key (big_dpart_dft,\"values\",\"table_example\")\n",
    "         big_dpart_dft_json_normalised = func_explode_based_on_depper_root_key(big_dpart_dft_json_normalised,[\"table_example\",\"entities\"],\"entities\")\n",
    "         func_cache_dft_data(big_dpart_dft_json_normalised)\n",
    "         print(func_get_rdd_num_partitions(big_dpart_dft_json_normalised))\n",
    "         big_dpart_dft_json_norm_renamed  = func_remane_column_names(big_dpart_dft_json_normalised)\n",
    "         processed_dft_row_count += finc_count_dft_rows(big_dpart_dft_json_norm_renamed)\n",
    "         func_write_data_to_parquet(parquet_file_path,big_dpart_dft_json_norm_renamed,\"append\")\n",
    "     return processed_dft_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    # json_input_row_count  = func_main () \n",
    "    big_dpart_parquet = func_read_parquet_file(parquet_file_path)\n",
    "    # parquet_row_count  = finc_count_dft_rows(big_dpart_parquet)\n",
    "    # big_dpart_parquet = func_limit_rows_dataframe(big_dpart_parquet,100)\n",
    "    big_dpart_parquet.show()\n",
    "except  Exception as error :\n",
    "    spark.stop()\n",
    "    raise error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = \"jdbc:postgresql://host.docker.internal:5432/crime_data_la\"\n",
    "db_properties = {\n",
    "    \"user\": \"john_user\",\n",
    "    \"password\": \"abc@12345\"\n",
    "}\n",
    "\n",
    "table_name = \"crime_data.transactions_data\"\n",
    "df_table = spark.read.jdbc(url=db_url, table=table_name, properties=db_properties)\n",
    "df_table.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dpart_parquet.select(big_dpart_parquet[\"table_example_entities\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_learning_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
